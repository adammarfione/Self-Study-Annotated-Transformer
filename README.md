# Self-Study-Annotated-Transformer
Self study project on "The Annotated Transformer" ( nlp.seas.harvard.edu/annotated-transformer/ ) 


"The Annotated Transformer" ( nlp.seas.harvard.edu/annotated-transformer/ ) is a PyTorch implementation of the Transformer Architecture from the 2017 paper, "Attention is all you need" ( https://arxiv.org/abs/1706.03762 ). The Origonal Author of "The Annotated Transformer" is Sasha Rush, and v2022 by Austin Huang, Suraj Subramanian, Jonathan Sum, Khalid Almubarak, & Stella Biderman. An excellent self-study project that pairs nicely with Jay Alammar's "The Illustrated Transformer" & "The Illustrated GPT-2 (Visualizing Transformer Language Models) ( https://jalammar.github.io/illustrated-transformer/ ).
